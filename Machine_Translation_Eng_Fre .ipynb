{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informed-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "august-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "breathing-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256\n",
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-martial",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m input_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m      4\u001b[0m output_characters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines[:\u001b[38;5;28mmin\u001b[39m(num_samples,\u001b[38;5;28mlen\u001b[39m(lines)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fra.txt'"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "output_texts = []\n",
    "input_characters = set()\n",
    "output_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, output_text, _ = line.split('\\t')\n",
    "    \n",
    "    # We use 'tab' as the 'start sequence' character\n",
    "    # for the targets, and '\\n' as the 'end sequence' character.\n",
    "    \n",
    "    output_text = '\\t' + output_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in output_text:\n",
    "        if char not in output_characters:\n",
    "            output_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earlier-sixth",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m num_encoder_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_characters)\n\u001b[0;32m      4\u001b[0m num_decoder_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output_characters)\n\u001b[1;32m----> 5\u001b[0m max_encoder_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m max_decoder_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m output_texts])\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "output_characters = sorted(list(output_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(output_characters)\n",
    "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in output_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greater-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 0\n",
      "Number of unique input Tokens: 0\n",
      "Number of unique output Tokens: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_encoder_seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of unique input Tokens:\u001b[39m\u001b[38;5;124m'\u001b[39m , num_encoder_tokens)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of unique output Tokens:\u001b[39m\u001b[38;5;124m'\u001b[39m , num_decoder_tokens)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax sequence length for inputs:\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[43mmax_encoder_seq_length\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMax sequence length for outputs:\u001b[39m\u001b[38;5;124m'\u001b[39m , max_decoder_seq_length)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_encoder_seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples:\", len(input_texts))\n",
    "print('Number of unique input Tokens:' , num_encoder_tokens)\n",
    "print('Number of unique output Tokens:' , num_decoder_tokens)\n",
    "print('Max sequence length for inputs:' , max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:' , max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hawaiian-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(input_characters)])\n",
    "\n",
    "output_token_index = dict(\n",
    "    [(char,i) for i,char in enumerate(output_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comic-bunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index , output_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deluxe-finnish",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_encoder_seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m encoder_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m----> 2\u001b[0m     (\u001b[38;5;28mlen\u001b[39m(input_texts),\u001b[43mmax_encoder_seq_length\u001b[49m,num_encoder_tokens),dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m decoder_input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;28mlen\u001b[39m(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m decoder_output_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;28mlen\u001b[39m(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_encoder_seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype = 'float32')\n",
    "decoder_output_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sought-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,output_text) in enumerate(zip(input_texts,output_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
    "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
    "    \n",
    "    for t, char in enumerate(output_text):\n",
    "        # decoder_output_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t,output_token_index[char]] = 1.\n",
    "        if t>0:\n",
    "            # decoder_output_data will be ahead by one timestep\n",
    "            # and will not include the start character\n",
    "            decoder_output_data[i,t-1,output_token_index[char]] = 1.\n",
    "    decoder_input_data[i,t+1:, output_token_index[' ']] = 1.\n",
    "    decoder_output_data[i , t: , output_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sophisticated-singapore",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mencoder_input_data\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "backed-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an input sequence and process it:\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state= True)\n",
    "encoder_outputs,state_h,state_c = encoder(encoder_inputs)\n",
    "# we discard 'encoder_outputs' and only keep the states.\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "outdoor-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the decoder , using encoder_states as initial states:\n",
    "decoder_inputs= Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim,return_sequences = True, return_state = True)\n",
    "decoder_outputs, _ ,_ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens,activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "involved-tribute",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Run training:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m , metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([\u001b[43mencoder_input_data\u001b[49m,decoder_input_data], decoder_output_data,\n\u001b[0;32m      8\u001b[0m          batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m      9\u001b[0m          epochs \u001b[38;5;241m=\u001b[39m epochs,\n\u001b[0;32m     10\u001b[0m          validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder_input_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# 'encoder_input_data' & 'decoder_input_data' into 'decoder_output_data'\n",
    "model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
    "\n",
    "#Run training:\n",
    "model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "model.fit([encoder_input_data,decoder_input_data], decoder_output_data,\n",
    "         batch_size=batch_size,\n",
    "         epochs = epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling models:\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape = (latent_dim,))\n",
    "decoder_state_input_c = Input(shape = (latent_dim,))\n",
    "decoder_input_states = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "decoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs, initial_state=decoder_input_states)\n",
    "decoder_states = [state_h,state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs]+decoder_input_states,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i,char) for char,i in input_token_index.items())\n",
    "reverse_output_char_index = dict(\n",
    "    (i,char) for char,i in output_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value= encoder_model.predict(input_seq)\n",
    "    output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "    output_seq[0,0,output_token_index['\\t']] = 1\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentences = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict(\n",
    "            [output_seq]+ states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1, :])\n",
    "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
    "        decoded_sentences += sampled_char\n",
    "\n",
    "        if(sampled_char == '\\n' or len(decoded_sentences) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        \n",
    "        #update the target sequence (of length 1):\n",
    "        output_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        output_seq[0,0,sampled_token_index] = 1\n",
    "        \n",
    "        states_value = [h,c]\n",
    "    return decoded_sentences\n",
    "\n",
    "for seq_index in range(20):\n",
    "    # take one sequence for trying out decoding:\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentences = decode_sequence(input_seq)\n",
    "    print('langauge translator Project: English to French Translation ')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:' , decoded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fdc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c2493e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
